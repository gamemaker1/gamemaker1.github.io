<!doctype html>
<html>
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <script src="https://cdn.tailwindcss.com"></script>
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Roboto+Flex:opsz,wdth@8..144,112.5&family=Roboto+Slab&display=swap"
            rel="stylesheet"
        />
        <style>
            .roboto-regular {
                font-family: 'Roboto Slab', sans-serif;
                font-weight: 400;
                font-style: normal;
            }
            .robotoslab-medium {
                font-family: 'Roboto Slab', sans-serif;
                font-weight: 500;
                font-style: normal;
            }
        </style>
    </head>

    <body class="roboto-regular">
        <div class="bg-gray-100 font-sans">
            <div class="container mx-auto py-8 px-4">
                <div class="bg-white p-6 rounded-lg shadow-lg">
                    <h1 class="text-3xl font-semibold robotoslab-regular">
                        Offline Function Calling
                    </h1>
                    <i class="text-gray-600 text-sm">
                        <a
                            href="https://offline-function-calling.github.io"
                            class="hover:underline"
                        >offline-function-calling.github.io</a>
                    </i> |
                    <i class="text-gray-600 text-sm">
                        <a
                            href="https://summerofcode.withgoogle.com/programs/2025/projects/rexKK7eu"
                            class="hover:underline"
                        >a GSoC project</a>
                        by <a href="https://gamemaker1.github.io" class="hover:underline">
                            Vedant Kulkarni
                        </a>
                    </i>

                    <hr class="my-4" />

                    <h2 class="text-xl robotoslab-medium mb-2" id="background">
                        Background
                    </h2>
                    <p class="text-gray-700 text-sm">
                        This project was started as part of my <a
                            class="text-blue-400 hover:underline"
                            href="https://summerofcode.withgoogle.com/programs/2025/projects/rexKK7eu"
                        >Google Summer of Code 2025 project with Google DeepMind</a>.
                        The primary goal was to explore, extend, and document the
                        function calling capabilities of the Gemma model family,
                        This effort included benchmarking and writing tutorials
                        and cookbooks for developers working with offline models
                        like Gemma 3.
                    </p>
                    <p class="text-gray-700 text-sm mt-2">
                        Over 12 weeks, the project progressed from <a
                            class="text-blue-400 hover:underline"
                            href="https://github.com/gamemaker1/gemma3-function-calling-experiments"
                        >simple initial experiments</a> to creating a <a
                            class="text-blue-400 hover:underline"
                            href="https://github.com/offline-function-calling/benchmarks"
                        >comprehensive benchmarking suite</a>, to the development
                        of a <a
                            class="text-blue-400 hover:underline"
                            href="https://github.com/offline-function-calling/sdk"
                        >function calling SDK</a>
                        and a <a
                            class="text-blue-400 hover:underline"
                            href="https://github.com/offline-function-calling/cli"
                        >command-line interface</a> not unlike the Gemini CLI.
                        This effort included writing and refining <a
                            class="text-blue-400 hover:underline"
                            href="https://github.com/offline-function-calling/sdk/tree/main/docs/learn"
                        >tutorials</a>,
                        designing and running <a
                            class="text-blue-400 hover:underline"
                            href="https://github.com/offline-function-calling/benchmarks"
                        >benchmarks</a>, and ongoing efforts to support function
                        calling via the Ollama API in Gemma 3. The goal was to
                        create a set of resources that demonstrates the
                        viability of function calling with offline, open-source
                        models, and helps developers get started with it.
                    </p>

                    <hr class="my-4" />

                    <h2 class="text-xl robotoslab-medium mt-4 mb-2" id="contributions">
                        Contributions
                    </h2>

                    <div class="mb-4 mt-4">
                        <h3 class="text-lg robotoslab-medium" id="implementing-function-calling">
                            Implementing Function Calling in Gemma 3
                        </h3>
                        <p class="mt-2 text-gray-700 text-sm">
                            A framework for function calling was designed and
                            implemented for the Gemma 3 family of models. The
                            framework consists of 5 key parts - instruction,
                            discovery, calling, parsing and execution - each
                            of which is explained in detail in the <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/hello-world"
                            >introductory tutorial.</a> The framework's design
                            evolved significantly over the course of the project.
                            Two key architectural innovations emerged, detailed in
                            the <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/structuring-and-scaling/"
                            >second tutorial</a>:
                        </p>
                        <ul class="list-disc list-inside text-gray-700 text-sm ml-4 mt-2">
                            <li>
                                Adoption of a <a
                                    class="text-blue-400 hover:underline"
                                    href="https://offline-function-calling.github.io/schemas/function.schema.json"
                                >schema based approach</a> to function
                                discovery, with extensions to the OpenAPI-like
                                schema that included all possible responses and
                                errors that a function could return.
                            </li>
                            <li>
                                Playing to the model's strength and using markdown
                                code blocks to wrap the function specifications and
                                calls, which helped Gemma models produce syntactically
                                valid JSON function calls, with unique identifiers.
                                This enables reliable asynchronous execution and
                                accurate pairing of calls with their outputs even
                                when models generate multiple function calls in parallel.
                            </li>
                        </ul>
                        <p class="mt-2 text-gray-700 text-sm">
                            The framework was progressively extended to support
                            increasingly sophisticated use cases. <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/multimodal-input/"
                            >Multimodal input support</a> was experimented with
                            and proved to succesfully handle complex scenarios
                            like extracting expense parameters from receipt images,
                            demonstrating practical applications where structured
                            data extraction from visual inputs drives function
                            execution. The <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/dynamic-function-generation/"
                            >dynamic function generation</a> capability enabled
                            models to write, register, and execute their own code
                            within a secure <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/microsandbox/microsandbox"
                            >microsandbox</a> environment when existing functions
                            do not meet the needs of the user.
                        </p>
                        <p class="mt-2 text-gray-700 text-sm">
                            Efforts were also devoted to exploring and pushing
                            the limits of different Gemma model sizes and
                            quantizations. While larger models (with 12B and 27B
                            parameters) handled complex multi-turn conversations
                            and large function sets effectively, smaller models
                            (1B and 4B) showed rapid degradation in performance
                            beyond 3-4 function calls. Investigations into Gemma
                            3n models for audio processing revealed challenges
                            with the models' ability to simultaneously process
                            large function specification texts and audio
                            instructions, as documented in a <a
                                class="text-blue-400 hover:underline"
                                href="https://gist.github.com/gamemaker1/341ca024645c1c95e846cf85a9aaffd1"
                            >minimal reproduction</a>. These findings informed
                            the development of model-specific prompting strategies
                            and highlighted areas for future fine-tuning work.
                        </p>
                    </div>

                    <div class="mb-4 mt-4">
                        <h3 class="text-lg robotoslab-medium" id="benchmarks-and-leaderboard">
                            Benchmarks and Leaderboard
                        </h3>
                        <p class="mt-2 text-gray-700 text-sm">
                            To objectively measure and compare function calling
                            capabilities across different models, a <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/offline-function-calling/benchmarks"
                            >comprehensive benchmark suite</a> was developed.
                            The creation process involved research into existing
                            evaluation methodologies and benchmarks, such as the <a
                                class="text-blue-400 hover:underline"
                                href="https://gorilla.cs.berkeley.edu/leaderboard.html"
                            >Berkeley Function Calling Leaderboard</a>, and
                            building on them. The suite's <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/bench/methodology/"
                            >methodology</a> systematically evaluates models
                            against 15 distinct parameters including parameter
                            transformation, error handling, composite calling,
                            parallel execution, and more, across 10 scenarios.
                            It evaluates the models with prompts and settings
                            that enable it to do its best, and also test the
                            ability of the model to use the tools effectively
                            in conversation, rather than just testing its ability
                            to produce accurate function calls.
                        </p>
                        <p class="mt-2 text-gray-700 text-sm">
                            Test execution is fully automated using <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/promptfoo/promptfoo"
                            >Promptfoo</a> as the test runner, with <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/ollama/ollama"
                            >Ollama</a> serving as the model provider for
                            local execution. LLM-based graders such as
                            Gemini 2.5 Flash were also used for evaluation
                            where the intent or non-deterministic output of
                            the model was to be tested.
                        </p>
                        <p class="mt-2 text-gray-700 text-sm">
                            A <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/offline-function-calling/benchmarks/blob/main/scripts/report.js"
                            >custom analysis script</a> processes raw test results,
                            normalizes scores across different parameters, and
                            generates a <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/bench/leaderboard/"
                            >leaderboard</a> comparing the function calling
                            abilities of 25 different Gemma model variants and
                            quantizations. The leaderboard presents detailed
                            breakdowns including normalized scores for each of
                            the 15 parameters tested, average latency measurements,
                            and qualitative feedback from LLM graders for each
                            scenario. This revealed insights about how different
                            parameter counts and quantizations affected function
                            calling capabilities - higher parameter count models
                            (12B and 27B) substantially outperform smaller variants
                            (1B and 4B) in function calling tasks, while QAT
                            models generally demonstrate slightly better instruction
                            following than their quantized counterparts.
                        </p>
                        <p class="mt-2 text-gray-700 text-sm">
                            The scoring methodology underwent significant
                            refinement to improve accuracy and granularity.
                            Initially, parameter scores were assigned based on
                            overall scenario performance, leading to coarse
                            grained evaluation where a model failing one aspect
                            of a multi-parameter scenario would receive poor
                            scores across all tested parameters. The system was <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/offline-function-calling/benchmarks/commit/09361bf3bcd43bf2757f15231379bfe9d7ab017e"
                            >enhanced to associate each test case directly with specific parameters</a>,
                            enabling fine-grained analysis of exactly where
                            models succeed or fail in the function calling
                            process.
                        </p>
                    </div>

                    <!--
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 p-4">
                      <figure class="bg-white border border-gray-200 rounded-lg shadow-md overflow-hidden">
                        <img class="w-full h-auto" src="./media/leaderboard.png" alt="Screenshot of the project leaderboard">
                        <figcaption class="p-4 text-center text-xs text-gray-700">
                          The leaderboard compares different models based on the number of scenarios passed and the normalized parameter scores. It also computes a composite score that includes the latency.
                        </figcaption>
                      </figure>

                      <figure class="bg-white border border-gray-200 rounded-lg shadow-md overflow-hidden">
                        <img class="w-full h-auto" src="./media/breakdown.png" alt="Screenshot of the benchmark evaluation details">
                        <figcaption class="p-4 text-center text-xs text-gray-700">
                          The leaderboard interface also provides the scores of the model on each parameter, and a list of tests along with the rationale behind the test, the functions and prompts, and the expected output.
                        </figcaption>
                      </figure>
                    </div>
                    -->

                    <div class="mb-4 mt-4">
                        <h3 class="text-lg robotoslab-medium" id="documentation-and-tutorials">
                            Writing Tutorials and Documentation
                        </h3>
                        <p class="mt-2 text-gray-700 text-sm">
                            Creating comprehensive, accessible documentation
                            was a primary goal of the project to make offline
                            function calling easy to get started with. The
                            documentation process evolved from <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/gamemaker1/gemma3-function-calling-tutorial"
                            >Jupyter notebook tutorials and cookbooks</a> into
                            an educational resource that first establishes basic
                            concepts and then <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/hello-world/"
                            >guides developers through a basic implementation</a>
                            of function calling. This is followed by guides on
                            <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/structuring-and-scaling/"
                            >how to extend the implementation to be structured and scalable</a>,
                            <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/multimodal-input/"
                            >how to support multimodal input</a>, and <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/learn/dynamic-function-generation/"
                            >how to enable dynamic function generation</a>.
                            The multimodal tutorial demonstrates practical
                            applications like building an expense tracker that
                            can process receipt images, while the dynamic function
                            generation guide shows how models can write and
                            execute functions on their own within secure,
                            sandboxed environments.
                        </p>
                        <p class="mt-2 text-gray-700 text-sm">
                            The documentation also includes guides on <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/setup/models"
                            >how to setup and use common tools</a> such as Python,
                            Ollama, and Microsandbox, as well as the <a
                                class="text-blue-400 hover:underline"
                                href="https://offline-function-calling.github.io/setup/cli"
                            >Offline Function Calling CLI</a>.
                        </p>
                    </div>

                    <div class="mb-4 mt-4">
                        <h3 class="text-lg robotoslab-medium" id="sdk-and-cli">
                            Creating a SDK and CLI
                        </h3>
                        <p class="mt-2 text-gray-700 text-sm">
                            To make it easier to get started with using the
                            models and framework outlined in the tutorials,
                            the <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/offline-function-calling/sdk"
                            >Offline Function Calling SDK</a> and <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/offline-function-calling/cli"
                            >CLI</a> were created. The CLI uses the SDK.
                        </p>
                        <ul class="list-disc list-inside text-gray-700 text-sm ml-4 mt-2">
                            <li>
                                The SDK provides an easy to use API for function
                                calling with offline models. The architecture
                                features a provider-based design supporting multiple
                                backends including <a
                                    class="text-blue-400 hover:underline"
                                    href="https://github.com/ollama/ollama"
                                >Ollama</a>, with planned support for HuggingFace
                                Transformers and MLX. The API draws inspiration
                                from the <a
                                    class="text-blue-400 hover:underline"
                                    href="https://a2a-protocol.org/latest/specification/"
                                >A2A protocol</a>, facilitating its use in future
                                agent-based applications. The SDK incorporates all
                                the functionality discovered through the course of
                                the project, including prompting strategies, function
                                discovery mechanisms, code generation and specification
                                creation, function call parsing and execution, and
                                robust model interaction patterns.
                            </li>
                            <li>
                                The user-friendly CLI enables seamless interaction with
                                offline function calling models. It makes creating and
                                using tools with offline models extremely easy - users
                                can simply place Python files containing their tools in
                                a designated directory, and the CLI automatically discovers
                                and registers functions based on their docstrings and
                                type hints. The CLI also provides comprehensive multimodal
                                input support via the models' image and audio capabilities,
                                as well as the <a
                                    class="text-blue-400 hover:underline"
                                    href="https://github.com/microsoft/markitdown"
                                >markitdown</a> library for processing documents.
                            </li>
                        </ul>
                        <p class="mt-2 text-gray-700 text-sm">
                            Efforts were also taken to integrate the function
                            calling capabilities of the Gema 3 models into existing
                            tools such as Ollama. It was demonstrated that tool
                            calls using the Ollama API worked with the official
                            Gemma model, provided a <a
                                class="text-blue-400 hover:underline"
                                href="http://github.com/offline-function-calling/cli/tree/main/models"
                            >custom Modelfile</a>. The model was able to discover
                            and use upto 20 tools in large context conversations.
                            While this integration remains under consideration,
                            the work established that the model satisfies the
                            compatibility requirements.
                        </p>
                    </div>

                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 p-4">
                      <figure class="bg-white border border-gray-200 rounded-lg shadow-md overflow-hidden">
                        <img class="w-full h-auto" src="./media/weather.png" alt="CLI using the get_weather tool">
                        <figcaption class="p-4 text-center text-xs text-gray-700">
                          The CLI in action, using a tool to answer the user's question.
                        </figcaption>
                      </figure>
  
                      <figure class="bg-white border border-gray-200 rounded-lg shadow-md overflow-hidden">
                        <img class="w-full h-auto" src="./media/expenses.png" alt="CLI using the record_expense tool from an image">
                        <figcaption class="p-4 text-center text-xs text-gray-700">
                          A demo of multimodal capabilities, where the model calls a tool based on data from an image.
                        </figcaption>
                      </figure>
                    </div>

                    <div class="mb-4 mt-4">
                        <h3 class="text-lg robotoslab-medium" id="fine-tuning">
                            Fine-Tuning
                        </h3>
                        <p class="mt-2 text-gray-700 text-sm">
                            Addressing the performance limitations of smaller,
                            more resource-efficient models became another goal
                            of the project, as the benchmarks revealed significant
                            capability gaps between different parameter sizes.
                            Benchmark results consistently showed that while
                            larger models (12B and 27B parameters) excelled at
                            function calling tasks, smaller variants (1B and 4B)
                            struggled with multi-turn conversations and complex
                            function calls, limiting their practical deployment
                            in resource-constrained environments.
                        </p>
                        <p class="mt-2 text-gray-700 text-sm">
                            To bridge this gap, a dataset for function calling
                            training is being constructed by combining the
                            scenarios from the benchmark suite with the BFCLv3
                            corpus. This dataset, once completed, will represent
                            a comprehensive collection of function calling
                            patterns that evaluate the model based on a variety
                            of parameters and scenarios. It can be used for
                            fine-tuning and enhancing the reasoning and function
                            calling capabilities of smaller Gemma 3 models (1B, 4B)
                            and the Gemma 3n variants (E2B, E4B) using the <a
                                class="text-blue-400 hover:underline"
                                href="https://github.com/unslothai/unsloth"
                            >Unsloth</a> library for optimized training performance.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>
